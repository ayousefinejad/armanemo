{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axWWbHc_OHL6",
        "outputId": "7c1f7c80-4bdc-4ab7-c238-e93e27caf549"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PQwA9NTjOEjX"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.chdir('/mnt/disk2/arshia.yousefinezhad/emotion_detection')\n",
        "# os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6JQ7eZDqOEjb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "from transformers import BertConfig, BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mNXknNuZOEjb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "max_len = 64\n",
        "train_batch_size = 64\n",
        "valid_batch_size = 64\n",
        "test_batch_size = 64\n",
        "\n",
        "epoch = 3\n",
        "EEVERY_EPOCH = 1000\n",
        "lr = 2e-5\n",
        "CLIP = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RlLcbzObOEjc"
      },
      "outputs": [],
      "source": [
        "# def label_ecoding(df):\n",
        "#     df.emotion.replace([' عاشقانه', 'عاشقانه', 'عاشقانه '], 'عاشقانه', inplace=True)\n",
        "#     df.emotion.replace(['خوشال','خوشحال','خوشحال ','خوشحالی'], 'خوشحال', inplace=True)\n",
        "#     df.emotion.replace(['هییجان انگیز ',  'هیجانزده',  'هیجان زده ', 'هیجان زده', 'هیجان انگیز ', 'هیجان انگیز', 'هیجان',\n",
        "#                         'شگفت زده', 'تعجب','متعجب','با تمام مشکلات خودمو کشیدم بالا و گوشمو تابوندم هر کسی رو محرم خودم ندونم+D23901:D23906'],\n",
        "#                         'هیجانی و متعجب', inplace=True)\n",
        "#     df.emotion.replace(['نارحت', 'ناراحتی', 'ناراحت ', 'ناراحت', 'مناراحت', 'غمگین','نگران'], 'غمگین', inplace=True)\n",
        "#     df.emotion.replace(['مضطرب','اضطراب'], 'مضطرب', inplace=True)\n",
        "#     df.emotion.replace(['معمول','معمولا','معمولب','معمولی','معمولی ','معولی','ممعمولی','ممولی'], 'معمولی', inplace=True)\n",
        "#     df.emotion.replace([ 'عصبانی','عصبانی ','اعصبانی','خشم', 'اعتراض', 'اعتراضی', 'تنفر'], 'عصبانی', inplace=True)\n",
        "#     df.emotion.replace(['ترس','ترس ','ترسناک'], 'ترس', inplace=True)\n",
        "\n",
        "#     df = df[~df['emotion'].isin(['رس','شرمندگی', 'نترس', 'اعتراضی'])]\n",
        "#     return df\n",
        "\n",
        "# def preprocess_text(df):\n",
        "#     def process_text(text):\n",
        "#       text = re.sub(r'[^ا-ی!\\s]', '', text)\n",
        "#       text = re.sub('[0-9]','',text)\n",
        "#       return text\n",
        "#     df.combined_text = df.combined_text.apply(process_text)\n",
        "#     return df\n",
        "data = pd.read_csv('/content/drive/MyDrive/#University_Tehran/train_process.csv')\n",
        "# data_prepared = preprocess_text(data)\n",
        "\n",
        "# data_prepared_labels = label_ecoding(data_prepared)\n",
        "# data = data_prepared_labels.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cub-2OkKOEjd"
      },
      "source": [
        "# Data Loader File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NptyCeu3OEje"
      },
      "outputs": [],
      "source": [
        "class CallCenterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, contexts, targets=None, label_list=None, max_len=128):\n",
        "        self.contexts = contexts\n",
        "        self.targets = targets\n",
        "        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Create a label-to-index mapping if label_list is provided\n",
        "        self.label_map = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        context = str(self.contexts[item])\n",
        "        if self.has_target:\n",
        "            target = self.label_map.get(str(self.targets[item]), str(self.targets[item]))\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        inputs = {\n",
        "            'context': context,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "        }\n",
        "        if self.has_target:\n",
        "            inputs['targets'] = torch.tensor(target, dtype=torch.long)\n",
        "        return inputs\n",
        "\n",
        "def create_data_loader(x, y, tokenizer, max_len, batch_size, label_list):\n",
        "    dataset = CallCenterDataset(\n",
        "        contexts=x,\n",
        "        targets=y,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len,\n",
        "        label_list=label_list)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "def split_data(df):\n",
        "    labels_list = list(sorted(df['label'].unique()))\n",
        "    df['label_id'] = df['label'].apply(lambda t: labels_list.index(t))\n",
        "    train, test = train_test_split(df, test_size=0.1, random_state=1, stratify=df['label'])\n",
        "    train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['label'])\n",
        "    train = train.reset_index(drop=True)\n",
        "    valid = valid.reset_index(drop=True)\n",
        "    test = test.reset_index(drop=True)\n",
        "    return train, valid, test\n",
        "\n",
        "\n",
        "train, valid, test = split_data(data)\n",
        "\n",
        "x_train, y_train = train['text'].values.tolist(), train['label_id'].values.tolist()\n",
        "x_valid, y_valid = valid['text'].values.tolist(), valid['label_id'].values.tolist()\n",
        "x_test, y_test = test['text'].values.tolist(), test['label_id'].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82U7X8x7OEje"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjixdlsQOEjf"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K5AWma_dOEjf"
      },
      "outputs": [],
      "source": [
        "parsbert_id = 'HooshvareLab/bert-fa-base-uncased'\n",
        "\n",
        "label_list = list(sorted(data['label'].unique()))\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {v: k for k, v in label2id.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XzKVnhpOEjg"
      },
      "source": [
        "## ParsBert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToVmC-U3OEjg"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmsAsRvXOEjg",
        "outputId": "c6302986-314e-4b63-f2e0-ab53c517515c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(parsbert_id)\n",
        "bertcofig = BertConfig.from_pretrained(\n",
        "    parsbert_id, **{\n",
        "        'label2id': label2id,\n",
        "        'id2label': id2label,\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZGxpPWUOEjg"
      },
      "source": [
        "### create data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0u8VAFkfOEjg"
      },
      "outputs": [],
      "source": [
        "train_data_loader = create_data_loader(train['text'].to_numpy(), train['label'].to_numpy(), tokenizer, max_len, train_batch_size, label_list)\n",
        "valid_data_loader = create_data_loader(valid['text'].to_numpy(), valid['label'].to_numpy(), tokenizer, max_len, valid_batch_size, label_list)\n",
        "test_data_loader = create_data_loader(test['text'].to_numpy(), test['label'].to_numpy(), tokenizer, max_len, test_batch_size, label_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmVb_0p_OEjg"
      },
      "source": [
        "### Models artitecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C_H6Lbn-OEjh"
      },
      "outputs": [],
      "source": [
        "class TextClassificationLSTMBidirectional(nn.Module):\n",
        "    def __init__(self, config, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout):\n",
        "        super(TextClassificationLSTMBidirectional, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        bert_output = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "        # bert_output.last_hidden_state.size() = [batch_size, sen_len, 768]\n",
        "        embedded = bert_output.last_hidden_state.permute(1, 0, 2)\n",
        "        # embedded.size() =  [sen_len, batch_size, 768]\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        #output = [batch size, sent len, hid dim * num directions]\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7r3LPIzCmR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l10LPFWgOEjh"
      },
      "source": [
        "### Defining Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D5ToWvS0OEji"
      },
      "outputs": [],
      "source": [
        "# embedding_dim =\n",
        "hidden_dim = 128\n",
        "output_dim = len(label2id)\n",
        "pad_idx = 0\n",
        "n_epochs = 10\n",
        "lr = 1e-1\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(label2id)\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "\n",
        "\n",
        "model = TextClassificationLSTMBidirectional(\n",
        "                  bertcofig,\n",
        "                  EMBEDDING_DIM,\n",
        "                  HIDDEN_DIM,\n",
        "                  OUTPUT_DIM,\n",
        "                  N_LAYERS,\n",
        "                  BIDIRECTIONAL,\n",
        "                  DROPOUT).to(device)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count, total_loss = 0, 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_index, batch_data in enumerate(tqdm(dataloader)):\n",
        "\n",
        "        # Extract input features and labels from the batch_data dictionary\n",
        "        input_ids = batch_data['input_ids'].to(device)\n",
        "        attention_mask = batch_data['attention_mask'].to(device)\n",
        "        token_type_ids = batch_data['token_type_ids'].to(device)\n",
        "        targets = batch_data['targets'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predicted_targets = model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "        loss = criterion(predicted_targets, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_acc += (predicted_targets.argmax(1) == targets).sum().item()\n",
        "        total_count += targets.size(0)\n",
        "\n",
        "        if batch_index % log_interval == 0 and batch_index > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f}\".format(\n",
        "                    epoch, batch_index, len(dataloader), total_acc / total_count\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(dataloader):\n",
        "  model.eval()\n",
        "  total_acc, total_count = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_index, batch_data in enumerate(tqdm(dataloader)):\n",
        "        input_ids = batch_data['input_ids'].to(device)\n",
        "        attention_mask = batch_data['attention_mask'].to(device)\n",
        "        targets = batch_data['targets'].to(device)\n",
        "\n",
        "        predicted_targets = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion(predicted_targets, targets)\n",
        "        total_acc += (predicted_targets.argmax(1) == targets).sum().item()\n",
        "        total_count += targets.size(0)\n",
        "  return total_acc / total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqsF1xfjOEji"
      },
      "source": [
        "### Training ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kty_b4UOOEji",
        "outputId": "7e6e141d-768f-4ff7-af00-4be2335abb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs... :   0%|          | 0/10 [00:00<?, ?it/s]\n",
            "  0%|          | 0/77 [00:00<?, ?it/s]\u001b[A"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "\n",
        "for epoch in tqdm(range(1, 10 + 1), desc=\"Epochs... \"):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_data_loader)\n",
        "    accu_val = evaluate(valid_data_loader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "      total_accu = accu_val\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
        "        \"valid accuracy {:8.3f} \".format(\n",
        "            epoch, time.time() - epoch_start_time, accu_val\n",
        "        )\n",
        "    )\n",
        "    print(\"-\" * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TtCg838OEjj"
      },
      "source": [
        "### Save pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPqq8WOKOEjj"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/#Part/Emotion_Detection/emotion_detection_parsbert.pt')\n",
        "\n",
        "print(\"Checking the results of test dataset.\")\n",
        "accu_test = evaluate(test_data_loader)\n",
        "print(\"test accuracy {:8.3f}\".format(accu_test))\n",
        "\n",
        "evaluate(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELWT2bx3OEjj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrnPwCw4OEjj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSvanh2TOEjj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}