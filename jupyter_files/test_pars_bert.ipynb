{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hazm\n",
    "# !pip install clean-text\n",
    "# !pip install -U kaleido\n",
    "# # !pip install catboost\n",
    "\n",
    "# !pip install -q transformers\n",
    "# !pip install -q hazm\n",
    "# !pip install -q clean-text[gpl]\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# from hazm import Stemmer, word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, make_scorer, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import hazm\n",
    "from cleantext import clean\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "data = label_ecoding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_len_by_words'] = data['combined_text'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "min_max_len = data[\"text_len_by_words\"].min(), data[\"text_len_by_words\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = list(sorted(data['emotion'].unique()))\n",
    "\n",
    "data['emotion_id'] = data['emotion'].apply(lambda t: emotions.index(t))\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.1, random_state=1, stratify=data['emotion'])\n",
    "train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['emotion'])\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['combined_text'].values.tolist(), train['emotion_id'].values.tolist()\n",
    "x_valid, y_valid = valid['combined_text'].values.tolist(), valid['emotion_id'].values.tolist()\n",
    "x_test, y_test = test['combined_text'].values.tolist(), test['emotion_id'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "max_len = 128\n",
    "train_batch_size = 16\n",
    "valid_batch_size = 16\n",
    "test_batch_size = 16\n",
    "\n",
    "num_epochs = 3\n",
    "lr = 2e-5\n",
    "\n",
    "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
    "OUTPUT_PATH = '/content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a key finder based on label 2 id and id to label\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(emotions)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(f'label2id: {label2id}')\n",
    "print(f'id2label: {id2label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the tokenizer and configuration\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, **{\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "    })\n",
    "\n",
    "print(config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(train))\n",
    "sample_comment = train.iloc[idx]['combined_text']\n",
    "sample_label = train.iloc[idx]['emotion']\n",
    "\n",
    "print(f'Sample: \\n{sample_comment}\\n{sample_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sample_comment)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f'  context: {sample_comment}')\n",
    "print(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    sample_comment,\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(f'Keys: {encoding.keys()}\\n')\n",
    "for k in encoding.keys():\n",
    "    print(f'{k}:\\n{encoding[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_detection.loader import create_data_loader\n",
    "\n",
    "label_list = ['ترسناک', 'تنفر', 'خوشحال', 'عاشقانه', 'عصبانی', 'غمگین', 'متعجب', 'مضطرب', 'معمولی', 'هیجان انگیز']\n",
    "train_data_loader = create_data_loader(train['combined_text'].to_numpy(), train['emotion'].to_numpy(), tokenizer, MAX_LEN, TRAIN_BATCH_SIZE, label_list)\n",
    "valid_data_loader = create_data_loader(valid['combined_text'].to_numpy(), valid['emotion'].to_numpy(), tokenizer, MAX_LEN, VALID_BATCH_SIZE, label_list)\n",
    "test_data_loader = create_data_loader(test['combined_text'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(train_data_loader))\n",
    "\n",
    "print(sample_data.keys())\n",
    "\n",
    "print(sample_data['context'])\n",
    "print(sample_data['input_ids'].shape)\n",
    "print(sample_data['input_ids'][0, :])\n",
    "print(sample_data['attention_mask'].shape)\n",
    "print(sample_data['attention_mask'][0, :])\n",
    "print(sample_data['token_type_ids'].shape)\n",
    "print(sample_data['token_type_ids'][0, :])\n",
    "print(sample_data['targets'].shape)\n",
    "print(sample_data['targets'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_detection.models import TextClassificationModel\n",
    "model = TextClassificationModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "import collections\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "pt_model = None\n",
    "\n",
    "pt_model = TextClassificationModel(config=config)\n",
    "pt_model = pt_model.to(device)\n",
    "\n",
    "print('pt_model', type(pt_model))\n",
    "\n",
    "\n",
    "# sample data output\n",
    "\n",
    "sample_data_context = sample_data['context']\n",
    "sample_data_input_ids = sample_data['input_ids']\n",
    "sample_data_attention_mask = sample_data['attention_mask']\n",
    "sample_data_token_type_ids = sample_data['token_type_ids']\n",
    "sample_data_targets = sample_data['targets']\n",
    "\n",
    "# available for using in GPU\n",
    "sample_data_input_ids = sample_data_input_ids.to(device)\n",
    "sample_data_attention_mask = sample_data_attention_mask.to(device)\n",
    "sample_data_token_type_ids = sample_data_token_type_ids.to(device)\n",
    "sample_data_targets = sample_data_targets.to(device)\n",
    "\n",
    "\n",
    "outputs = F.softmax(\n",
    "    pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids),\n",
    "    dim=1)\n",
    "\n",
    "outputs = pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids)\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "print(outputs[:5, :])\n",
    "print(preds[:5])\n",
    "\n",
    "\n",
    "\n",
    "def simple_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).mean()\n",
    "\n",
    "def acc_and_f1(y_true, y_pred, average='weighted'):\n",
    "    acc = simple_accuracy(y_true, y_pred)\n",
    "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def y_loss(y_true, y_pred, losses):\n",
    "    y_true = torch.stack(y_true).cpu().detach().numpy()\n",
    "    y_pred = torch.stack(y_pred).cpu().detach().numpy()\n",
    "    y = [y_true, y_pred]\n",
    "    loss = np.mean(losses)\n",
    "\n",
    "    return y, loss\n",
    "\n",
    "\n",
    "def eval_op(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dl in tqdm(data_loader, total=len(data_loader), desc=\"Evaluation... \"):\n",
    "\n",
    "            input_ids = dl['input_ids']\n",
    "            attention_mask = dl['attention_mask']\n",
    "            token_type_ids = dl['token_type_ids']\n",
    "            targets = dl['targets']\n",
    "\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "            # convert output probabilities to predicted class\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # accumulate all the losses\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(targets)\n",
    "\n",
    "    eval_y, eval_loss = y_loss(y_true, y_pred, losses)\n",
    "    return eval_y, eval_loss\n",
    "\n",
    "\n",
    "def train_op(model,\n",
    "             data_loader,\n",
    "             loss_fn,\n",
    "             optimizer,\n",
    "             scheduler,\n",
    "             step=0,\n",
    "             print_every_step=1000,\n",
    "             eval=False,\n",
    "             eval_cb=None,\n",
    "             eval_loss_min=np.Inf,\n",
    "             eval_data_loader=None,\n",
    "             clip=0.0):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for dl in tqdm(data_loader, total=len(data_loader), desc=\"Training... \"):\n",
    "        step += 1\n",
    "\n",
    "        input_ids = dl['input_ids']\n",
    "        attention_mask = dl['attention_mask']\n",
    "        token_type_ids = dl['token_type_ids']\n",
    "        targets = dl['targets']\n",
    "\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "\n",
    "        # convert output probabilities to predicted class\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # accumulate all the losses\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        if clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "\n",
    "        # perform optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # perform scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(targets)\n",
    "\n",
    "        if eval:\n",
    "            train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
    "            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
    "\n",
    "            if step % print_every_step == 0:\n",
    "                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)\n",
    "                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
    "\n",
    "                if hasattr(eval_cb, '__call__'):\n",
    "                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)\n",
    "\n",
    "    train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
    "\n",
    "    return train_y, train_loss, step, eval_loss_min\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(pt_model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "step = 0\n",
    "eval_loss_min = np.Inf\n",
    "history = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "def eval_callback(epoch, epochs, output_path):\n",
    "    def eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min):\n",
    "        statement = ''\n",
    "        statement += 'Epoch: {}/{}...'.format(epoch, epochs)\n",
    "        statement += 'Step: {}...'.format(step)\n",
    "\n",
    "        statement += 'Train Loss: {:.6f}...'.format(train_loss)\n",
    "        statement += 'Train Acc: {:.3f}...'.format(train_score['acc'])\n",
    "\n",
    "        statement += 'Valid Loss: {:.6f}...'.format(eval_loss)\n",
    "        statement += 'Valid Acc: {:.3f}...'.format(eval_score['acc'])\n",
    "\n",
    "        print(statement)\n",
    "\n",
    "        if eval_loss <= eval_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                eval_loss_min,\n",
    "                eval_loss))\n",
    "\n",
    "            torch.save(model.state_dict(), output_path)\n",
    "            eval_loss_min = eval_loss\n",
    "\n",
    "        return eval_loss_min\n",
    "\n",
    "\n",
    "    return eval_cb\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs... \"):\n",
    "    train_y, train_loss, step, eval_loss_min = train_op(\n",
    "        model=pt_model,\n",
    "        data_loader=train_data_loader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        step=step,\n",
    "        print_every_step=EEVERY_EPOCH,\n",
    "        eval=True,\n",
    "        eval_cb=eval_callback(epoch, EPOCHS, OUTPUT_PATH),\n",
    "        eval_loss_min=eval_loss_min,\n",
    "        eval_data_loader=valid_data_loader,\n",
    "        clip=CLIP)\n",
    "\n",
    "    train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
    "\n",
    "    eval_y, eval_loss = eval_op(\n",
    "        model=pt_model,\n",
    "        data_loader=valid_data_loader,\n",
    "        loss_fn=loss_fn)\n",
    "\n",
    "    eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
    "\n",
    "    history['train_acc'].append(train_score['acc'])\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(eval_score['acc'])\n",
    "    history['val_loss'].append(eval_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, comments, tokenizer, max_len=128, batch_size=32):\n",
    "    data_loader = create_data_loader(comments, None, tokenizer, max_len, batch_size, None)\n",
    "\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for dl in tqdm(data_loader, position=0):\n",
    "            input_ids = dl['input_ids']\n",
    "            attention_mask = dl['attention_mask']\n",
    "            token_type_ids = dl['token_type_ids']\n",
    "\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "            # compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "            # convert output probabilities to predicted class\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(F.softmax(outputs, dim=1))\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu().detach().numpy()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
    "\n",
    "    return predictions, prediction_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = test['combined_text'].to_numpy()\n",
    "preds, probs = predict(pt_model, test_comments, tokenizer, max_len=128)\n",
    "\n",
    "print(preds.shape, probs.shape)\n",
    "\n",
    "\n",
    "y_test, y_pred = [label_list.index(label) for label in test['emotion'].values], preds\n",
    "\n",
    "\n",
    "print(f'F1: {f1_score(y_test, y_pred, average=\"weighted\")}')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=label_list))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
